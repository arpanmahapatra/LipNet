# -*- coding: utf-8 -*-
"""LipNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pTFMmXA2S-IWnPoJaebpuO6FeUlqIr-u
"""

import os
import cv2
import tensorflow as tf
import numpy as np
from typing import List
from typing import Tuple
from matplotlib import pyplot as plt
from tensorflow.keras.optimizers import legacy as optimizers
import imageio

devices = tf.config.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(devices[0], True)
except:
    pass

import gdown
url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'
output = 'data.zip'
gdown.download(url, output, quiet=False)

gdown.extractall('data.zip')

def load_vid(path:str) -> List[float]:

  cap = cv2.VideoCapture(path)
  frames = []
  for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):
    ret, frame = cap.read()
    frame = tf.image.rgb_to_grayscale(frame)
    frames.append(frame[190:236,80:220,:]) #slicing fucntion to isloate the mouth
  cap.release()

  mean = tf.math.reduce_mean(frames)
  std = tf.math.reduce_std(tf.cast(frames, tf.float32))
  return tf.cast((frames - mean), tf.float32) / std

vocab = [x for x in "abcdefghijklmnopqrstuvwxyz'?!123456789 "]

stoi = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token="")
itos = tf.keras.layers.StringLookup(vocabulary=stoi.get_vocabulary(), oov_token="", invert=True)

print(
    f"the vocab is {itos.get_vocabulary()} "
    f"(size {len(itos.get_vocabulary())})"
)

def load_alignments(path:str) -> List[str]:
  with open(path) as f:
    lines = f.readlines()
  tokens = []
  for line in lines:
    line = line.split()
    if line[2] != 'sil':
      tokens  = [*tokens, ' ', line[2]]
  return stoi(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]

def load_data(path: str):
    path = bytes.decode(path.numpy())

    fn = path.split('/')[-1].split('.')[0]
    video_path = os.path.join('data', 's1', f'{fn}.mpg')
    align_path = os.path.join('data', 'alignments', 's1', f'{fn}.align')  # Corrected path construction

    frames = load_vid(video_path)
    alignments = load_alignments(align_path)

    return frames, alignments

def mappable_f(path:str) ->List[str]:
    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))
    return result

data = tf.data.Dataset.list_files('./data/s1/*.mpg')
data = data.shuffle(500, reshuffle_each_iteration=False)
data = data.map(mappable_f)
data = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))
data = data.prefetch(tf.data.AUTOTUNE)
train = data.take(450)
test = data.skip(450)

frames, alignments = data.as_numpy_iterator().next()

len(frames)

sample = data.as_numpy_iterator()

val = sample.next(); val[0]

grayscale_frames = [(frame * 255).astype(np.uint8).squeeze() for frame in val[0][1]]
imageio.mimsave('./animation.gif', grayscale_frames, fps=10)

plt.imshow(val[0][0][0])

tf.strings.reduce_join([itos(word) for word in val[1][0]])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler

data.as_numpy_iterator().next()[0][0].shape

model = Sequential()
model.add(Conv3D(128, 3, input_shape=(75,46,140,1), padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(256, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(75, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1,2,2)))

model.add(TimeDistributed(Flatten()))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Dense(stoi.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))

model.summary()

yhat = model.predict(val[0])

tf.strings.reduce_join([itos(x) for x in tf.argmax(yhat[0],axis=1)])

yhat[0].shape

model.input

model.output

def scheduler(epoch, lr):
    if epoch < 30:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

def CTCLoss(y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")

    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    return loss

class ProduceExample(tf.keras.callbacks.Callback):
    def __init__(self, dataset) -> None:
        self.dataset = dataset.as_numpy_iterator()

    def on_epoch_end(self, epoch, logs=None) -> None:
        data = self.dataset.next()
        yhat = self.model.predict(data[0])
        decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()
        for x in range(len(yhat)):
            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))
            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))
            print('~'*100)

model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss=CTCLoss)

checkpoint_callback = ModelCheckpoint(os.path.join('models','checkpoint'), monitor='loss', save_weights_only=True)

schedule_callback = LearningRateScheduler(scheduler)

example_callback = ProduceExample(data)

model.fit(train, validation_data=test, epochs=100, callbacks=[checkpoint_callback, schedule_callback, example_callback])

import gdown
url = 'https://drive.google.com/uc?id=1vWscXs4Vt0a_1IH1-ct2TCgXAZT-N3_Y'
output = 'checkpoints.zip'
gdown.download(url, output, quiet=False)
gdown.extractall('checkpoints.zip', 'models')

from tensorflow.keras.optimizers import legacy as optimizers
model.load_weights('models/checkpoint')

test_data = test.as_numpy_iterator()

test_data.next()

sample = test_data.next()

yhat = model.predict(sample[0])

decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75,75], greedy=True)[0][0].numpy()

[tf.strings.reduce_join([itos(word) for word in sentence]) for sentence in decoded]

[tf.strings.reduce_join([itos(word) for word in sentence]) for sentence in sample[1]]

def preprocess_frame(frame):
    frame = tf.image.rgb_to_grayscale(frame)
    frame = frame[190:236, 80:220, :]  # slicing to isolate the mouth
    return frame

def normalize_frames(frames):
    mean = tf.math.reduce_mean(frames)
    std = tf.math.reduce_std(tf.cast(frames, tf.float32))
    return tf.cast((frames - mean), tf.float32) / std

def capture_from_webcam():
    cap = cv2.VideoCapture(-1)  # Open webcam
    frames = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        processed_frame = preprocess_frame(frame)
        frames.append(processed_frame)

        # Display the resulting frame
        cv2.imshow('Webcam', frame)

        # Predict transcription from frames every few frames
        if len(frames) % 30 == 0:  # Adjust this number as needed
            normalized_frames = normalize_frames(frames)
            predictions = predict_lip_reading(normalized_frames, model)
            print("Predicted transcription:", predictions)
            frames = []  # Clear frames to avoid memory overflow

        # Break the loop on 'q' key press
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

def predict_lip_reading(frames, model):
    frames = tf.expand_dims(frames, axis=0)
    yhat = model.predict(frames)
    decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()
    prediction = tf.strings.reduce_join([itos(word) for word in decoded]).numpy().decode('utf-8')
    return prediction

def main(model):
    capture_from_webcam()

main(model)